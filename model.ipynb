{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetsフォルダ内にあるcsvファイルの一覧を取得\n",
    "def get_data(dataset_dir:str, content:dict, t_range:int):\n",
    "  files = {num: glob.glob(f\"{dataset_dir}/{cont}/muscle*\") for num, cont in content.items()}\n",
    "  datas = [decode(file, num, t_range) for num, file in files.items()]\n",
    "  return assemble_table(datas)\n",
    "\n",
    "def decode(files:list, label:int, t_range:int):\n",
    "  name = lambda x: int(re.search(r\"\\d+\\.\", x).group(0).strip(\".\"))\n",
    "  table = {name(f): pd.read_csv(f, header=None, dtype=np.float32)[1] for f in files}\n",
    "  df = pd.DataFrame(table, index=np.arange(t_range)).T\n",
    "  l = pd.Series(np.full(100, label), name=1000, index=df.index)\n",
    "  return pd.concat([df,l], axis=1)\n",
    "\n",
    "def assemble_table(data:list):\n",
    "  return pd.concat(data, axis=0)\n",
    "\n",
    "# history plot\n",
    "def plot_history(fit):\n",
    "    fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "    # Plot the loss in the history\n",
    "    axL.plot(fit.history['loss'],label=\"loss for training\")\n",
    "    axL.plot(fit.history['val_loss'],label=\"loss for validation\")\n",
    "    axL.set_title('model loss')\n",
    "    axL.set_xlabel('epoch')\n",
    "    axL.set_ylabel('loss')\n",
    "    axL.legend(loc='upper right')\n",
    "\n",
    "    axR.plot(fit.history['accuracy'],label=\"loss for training\")\n",
    "    axR.plot(fit.history['val_accuracy'],label=\"loss for validation\")\n",
    "    axR.set_title('model accuracy')\n",
    "    axR.set_xlabel('epoch')\n",
    "    axR.set_ylabel('accuracy')\n",
    "    axR.legend(loc='upper right')\n",
    "    \n",
    "def preprocess(func, data):\n",
    "  d = data.copy()\n",
    "  d.loc[:,0:999] = d.loc[:,0:999].apply(func, axis=1)\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONST NUMBERS SETTING #####\n",
    "# Data settings\n",
    "dataset_dir = \"./Dataset/trial3\"\n",
    "content = {0:\"inside\", 1:\"outside\", 2:\"fist\"}\n",
    "t_range = 1000\n",
    "\n",
    "# Model settings\n",
    "batch_size = 16\n",
    "nb_epoch = 100\n",
    "nb_class = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Setup Train/Test Data #####\n",
    "# Load Data\n",
    "data = get_data(dataset_dir, content, t_range).sort_index()\n",
    "\n",
    "# Pre Processing\n",
    "# e.x.) \n",
    "pre1 = lambda x: pd.Series(fft(x))\n",
    "# pre2 = lambda x: pd.Series(np.log(x))\n",
    "data = preprocess(pre1, data)\n",
    "\n",
    "# Prepare train, test data\n",
    "ex_var = data.loc[:, 0:999].values\n",
    "obj_var = data.loc[:, 1000].values\n",
    "\n",
    "_x_train, _x_test, _y_train, _y_test = train_test_split(ex_var, obj_var, test_size=0.3, random_state=0, shuffle=True)\n",
    "\n",
    "x_train, y_train = _x_train.reshape(-1, 1000, 1), to_categorical(_y_train)\n",
    "x_test, y_test = _x_test.reshape(-1, 1000, 1), to_categorical(_y_test)\n",
    "\n",
    "print(f\"\"\"\n",
    "Dataset shape\n",
    "x_train: {x_train.shape} |-> y_train: {y_train.shape},\n",
    "x_test: {x_test.shape} |-> y_test: {y_test.shape}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model #####\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=128, input_shape=(1000,1), kernel_size=2, strides=1, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=2, strides=1, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=2, strides=1, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_class))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = \"rmsprop\",\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning\n",
    "if __name__==\"__main__\":\n",
    "  es_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "  fname = 'weights.{epoch:02d}-{loss:.2f}-{accuracy:.2f}-{val_loss:.2f}-{val_accuracy:.2f}.hdf5'\n",
    "  cp_cb = keras.callbacks.ModelCheckpoint(filepath=\"./checkout/\"+fname, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "  learning_rates = np.linspace(0.03, 0.001, nb_epoch)\n",
    "  lr_cb = keras.callbacks.LearningRateScheduler(lambda epoch: float(learning_rates[epoch]))\n",
    "\n",
    "  history = model.fit(x_train, y_train, batch_size=batch_size, epochs=nb_epoch, validation_data=(x_test, y_test), verbose=1, callbacks=[ cp_cb, lr_cb])\n",
    "  score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "  print(list(zip(model.metrics_names, score)))\n",
    "  plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "test = x_test[0:n], y_test[0:n]\n",
    "result = model.predict(test[0])\n",
    "\n",
    "for i in range(n):\n",
    "  p = np.argmax(result[i])\n",
    "  t = np.argmax(test[1][i])\n",
    "  print(f\"test{i}: predict...{content[p]} / ground truth...{content[t]}\\n  test{i}:{'○' if p == t else '☓'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myoelectricity",
   "language": "python",
   "name": "myoelectricity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
